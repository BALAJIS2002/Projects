{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#read the corpus(this is a sample file, upload task specific corpus in corpus text file and proceed)\n",
        "corpus=open(\"/content/sample_data/corpus.txt\").read()"
      ],
      "metadata": {
        "id": "aWB_bjtb8mkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "0ZH9KMsg8ziV",
        "outputId": "cff76c02-99a4-4366-f36c-2ca92038403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Next Word Prediction is also called Language Modeling. It is the task of predicting what word comes next. It is one of the fundamental tasks of NLP and has many applications. You might be using it daily when you write texts or emails without realizing it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the corpus\n",
        "import re\n",
        "corpus=corpus.lower()\n",
        "clean_corpus=re.sub('[^a-z0-9]+',' ', corpus)"
      ],
      "metadata": {
        "id": "90ASslle8zkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MtYm7E8S8znO",
        "outputId": "2ce4223b-5070-4bd0-9d01-83c709a4d704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'next word prediction is also called language modeling it is the task of predicting what word comes next it is one of the fundamental tasks of nlp and has many applications you might be using it daily when you write texts or emails without realizing it '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#required libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDjr__aw8zpy",
        "outputId": "3e9a2ede-70c1-43b2-93ea-a29bf74d770b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the text into words\n",
        "tokens = word_tokenize(clean_corpus)\n",
        "tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcqyfA7e8zsX",
        "outputId": "616d78c8-bb78-4094-d0ec-597219ceaf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['next',\n",
              " 'word',\n",
              " 'prediction',\n",
              " 'is',\n",
              " 'also',\n",
              " 'called',\n",
              " 'language',\n",
              " 'modeling',\n",
              " 'it',\n",
              " 'is',\n",
              " 'the',\n",
              " 'task',\n",
              " 'of',\n",
              " 'predicting',\n",
              " 'what',\n",
              " 'word',\n",
              " 'comes',\n",
              " 'next',\n",
              " 'it',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'fundamental',\n",
              " 'tasks',\n",
              " 'of',\n",
              " 'nlp',\n",
              " 'and',\n",
              " 'has',\n",
              " 'many',\n",
              " 'applications',\n",
              " 'you',\n",
              " 'might',\n",
              " 'be',\n",
              " 'using',\n",
              " 'it',\n",
              " 'daily',\n",
              " 'when',\n",
              " 'you',\n",
              " 'write',\n",
              " 'texts',\n",
              " 'or',\n",
              " 'emails',\n",
              " 'without',\n",
              " 'realizing',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#length of the sequence to train\n",
        "train_len = 3"
      ],
      "metadata": {
        "id": "0ydJ-7hh8zvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the data into required sequence\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)+1):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)"
      ],
      "metadata": {
        "id": "yMu3PzbR8zxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfvI5Yz28z0O",
        "outputId": "419e8803-673d-40f1-f561-ec4d8d0788dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['next', 'word', 'prediction'],\n",
              " ['word', 'prediction', 'is'],\n",
              " ['prediction', 'is', 'also'],\n",
              " ['is', 'also', 'called'],\n",
              " ['also', 'called', 'language'],\n",
              " ['called', 'language', 'modeling'],\n",
              " ['language', 'modeling', 'it'],\n",
              " ['modeling', 'it', 'is'],\n",
              " ['it', 'is', 'the'],\n",
              " ['is', 'the', 'task'],\n",
              " ['the', 'task', 'of'],\n",
              " ['task', 'of', 'predicting'],\n",
              " ['of', 'predicting', 'what'],\n",
              " ['predicting', 'what', 'word'],\n",
              " ['what', 'word', 'comes'],\n",
              " ['word', 'comes', 'next'],\n",
              " ['comes', 'next', 'it'],\n",
              " ['next', 'it', 'is'],\n",
              " ['it', 'is', 'one'],\n",
              " ['is', 'one', 'of'],\n",
              " ['one', 'of', 'the'],\n",
              " ['of', 'the', 'fundamental'],\n",
              " ['the', 'fundamental', 'tasks'],\n",
              " ['fundamental', 'tasks', 'of'],\n",
              " ['tasks', 'of', 'nlp'],\n",
              " ['of', 'nlp', 'and'],\n",
              " ['nlp', 'and', 'has'],\n",
              " ['and', 'has', 'many'],\n",
              " ['has', 'many', 'applications'],\n",
              " ['many', 'applications', 'you'],\n",
              " ['applications', 'you', 'might'],\n",
              " ['you', 'might', 'be'],\n",
              " ['might', 'be', 'using'],\n",
              " ['be', 'using', 'it'],\n",
              " ['using', 'it', 'daily'],\n",
              " ['it', 'daily', 'when'],\n",
              " ['daily', 'when', 'you'],\n",
              " ['when', 'you', 'write'],\n",
              " ['you', 'write', 'texts'],\n",
              " ['write', 'texts', 'or'],\n",
              " ['texts', 'or', 'emails'],\n",
              " ['or', 'emails', 'without'],\n",
              " ['emails', 'without', 'realizing'],\n",
              " ['without', 'realizing', 'it']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the texts into integer sequence\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPNzmYPz8z2z",
        "outputId": "87b7c460-be46-4a5f-e544-0b92eb3bff06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[7, 6, 8],\n",
              " [6, 8, 2],\n",
              " [8, 2, 9],\n",
              " [2, 9, 10],\n",
              " [9, 10, 11],\n",
              " [10, 11, 12],\n",
              " [11, 12, 1],\n",
              " [12, 1, 2],\n",
              " [1, 2, 4],\n",
              " [2, 4, 13],\n",
              " [4, 13, 3],\n",
              " [13, 3, 14],\n",
              " [3, 14, 15],\n",
              " [14, 15, 6],\n",
              " [15, 6, 16],\n",
              " [6, 16, 7],\n",
              " [16, 7, 1],\n",
              " [7, 1, 2],\n",
              " [1, 2, 17],\n",
              " [2, 17, 3],\n",
              " [17, 3, 4],\n",
              " [3, 4, 18],\n",
              " [4, 18, 19],\n",
              " [18, 19, 3],\n",
              " [19, 3, 20],\n",
              " [3, 20, 21],\n",
              " [20, 21, 22],\n",
              " [21, 22, 23],\n",
              " [22, 23, 24],\n",
              " [23, 24, 5],\n",
              " [24, 5, 25],\n",
              " [5, 25, 26],\n",
              " [25, 26, 27],\n",
              " [26, 27, 1],\n",
              " [27, 1, 28],\n",
              " [1, 28, 29],\n",
              " [28, 29, 5],\n",
              " [29, 5, 30],\n",
              " [5, 30, 31],\n",
              " [30, 31, 32],\n",
              " [31, 32, 33],\n",
              " [32, 33, 34],\n",
              " [33, 34, 35],\n",
              " [34, 35, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=np.asarray(sequences)"
      ],
      "metadata": {
        "id": "ZPa7QYvC8z5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk8gx2a-8z8T",
        "outputId": "a0bdb5be-980d-4fcb-ccc2-141839dee34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainX\n",
        "train_inputs=sequences[:,:-1]"
      ],
      "metadata": {
        "id": "7qcKLCvG8z-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYDG2VE80Bg",
        "outputId": "d7e6fd90-69df-4872-aea0-2320a243b70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7,  6],\n",
              "       [ 6,  8],\n",
              "       [ 8,  2],\n",
              "       [ 2,  9],\n",
              "       [ 9, 10],\n",
              "       [10, 11],\n",
              "       [11, 12],\n",
              "       [12,  1],\n",
              "       [ 1,  2],\n",
              "       [ 2,  4],\n",
              "       [ 4, 13],\n",
              "       [13,  3],\n",
              "       [ 3, 14],\n",
              "       [14, 15],\n",
              "       [15,  6],\n",
              "       [ 6, 16],\n",
              "       [16,  7],\n",
              "       [ 7,  1],\n",
              "       [ 1,  2],\n",
              "       [ 2, 17],\n",
              "       [17,  3],\n",
              "       [ 3,  4],\n",
              "       [ 4, 18],\n",
              "       [18, 19],\n",
              "       [19,  3],\n",
              "       [ 3, 20],\n",
              "       [20, 21],\n",
              "       [21, 22],\n",
              "       [22, 23],\n",
              "       [23, 24],\n",
              "       [24,  5],\n",
              "       [ 5, 25],\n",
              "       [25, 26],\n",
              "       [26, 27],\n",
              "       [27,  1],\n",
              "       [ 1, 28],\n",
              "       [28, 29],\n",
              "       [29,  5],\n",
              "       [ 5, 30],\n",
              "       [30, 31],\n",
              "       [31, 32],\n",
              "       [32, 33],\n",
              "       [33, 34],\n",
              "       [34, 35]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input sequence length\n",
        "seq_length=train_inputs.shape[1]\n",
        "seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1POR5QA580E6",
        "outputId": "9b920f14-3f20-42b7-a65a-ad8d8ed75915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainY\n",
        "train_targets=sequences[:,-1]"
      ],
      "metadata": {
        "id": "JmS16iM29IHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82ykUSv9IJb",
        "outputId": "6c27f32b-69b4-4ebf-937a-db0975d6120f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8,  2,  9, 10, 11, 12,  1,  2,  4, 13,  3, 14, 15,  6, 16,  7,  1,\n",
              "        2, 17,  3,  4, 18, 19,  3, 20, 21, 22, 23, 24,  5, 25, 26, 27,  1,\n",
              "       28, 29,  5, 30, 31, 32, 33, 34, 35,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)"
      ],
      "metadata": {
        "id": "3jFfq_HV9IL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zym49Wp9IOc",
        "outputId": "51f6b561-2c8a-4e43-fcbc-064702d2b1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#required libraries\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Q4_K9QNw9IRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm model\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        #simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=False)\n",
        "\n",
        "        #fully connected layer\n",
        "        self.linear = nn.Linear(hidden_size*seq_length,vocab_size)\n",
        "\n",
        "    def forward(self, input_word):\n",
        "        #input sequence to embeddings\n",
        "        embedded = self.embed(input_word)\n",
        "\n",
        "        #passing the embedding to lstm model\n",
        "        output, hidden = self.lstm(embedded)\n",
        "\n",
        "        #reshaping\n",
        "        output=output.view(output.size(0), -1)\n",
        "\n",
        "        #fully connected layer\n",
        "        output = self.linear(output)\n",
        "        return output,hidden\n"
      ],
      "metadata": {
        "id": "zSxX1PhC9IUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=lstm(vocab_size=vocabulary_size,embed_size=128, hidden_size=256)"
      ],
      "metadata": {
        "id": "cYIyiQH49IWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--hJSUbn9IZD",
        "outputId": "4d016886-0c65-4ad1-bbb0-179dd1224b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm(\n",
              "  (embed): Embedding(36, 128)\n",
              "  (lstm): LSTM(128, 256, num_layers=2)\n",
              "  (linear): Linear(in_features=512, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adam optimizer\n",
        "optimizer= Adam(model.parameters(), lr=0.07)\n",
        "\n",
        "#loss\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "SBZJH1B89Ica"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "def train(epoch):\n",
        "    #set the model to train\n",
        "    model.train()\n",
        "    tr_loss=0\n",
        "\n",
        "    #clearing the Gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #predict the output\n",
        "    y_pred, (state_h, state_c) = model(torch.from_numpy(train_inputs))\n",
        "\n",
        "    #compute the loss\n",
        "    loss=criterion(y_pred,torch.from_numpy(train_targets))\n",
        "    losses.append(loss)\n",
        "\n",
        "    #backpropagate\n",
        "    loss.backward()\n",
        "\n",
        "    #update the parameters\n",
        "    optimizer.step()\n",
        "    tr_loss = loss.item()\n",
        "\n",
        "    print(\"Epoch : \",epoch,\"loss : \",loss)"
      ],
      "metadata": {
        "id": "Kn6V6LbP9UDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of epoch\n",
        "no_epoch=50\n",
        "losses=[]\n",
        "for epoch in range(1,no_epoch+1):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDOAG-Mp9UGV",
        "outputId": "2d3bcbce-4cc6-4c45-bdaf-076db6dc7651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1 loss :  tensor(0.6898, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  2 loss :  tensor(0.4685, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  3 loss :  tensor(2.2208, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  4 loss :  tensor(0.9630, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  5 loss :  tensor(0.3622, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  6 loss :  tensor(0.4041, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  7 loss :  tensor(0.3771, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  8 loss :  tensor(0.3501, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  9 loss :  tensor(0.2809, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  10 loss :  tensor(0.2375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  11 loss :  tensor(0.1802, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  12 loss :  tensor(0.2744, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  13 loss :  tensor(0.1968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  14 loss :  tensor(0.2159, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  15 loss :  tensor(0.2248, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  16 loss :  tensor(0.2087, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  17 loss :  tensor(0.2097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  18 loss :  tensor(0.1940, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  19 loss :  tensor(0.2075, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  20 loss :  tensor(0.2014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  21 loss :  tensor(0.1986, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  22 loss :  tensor(0.2019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  23 loss :  tensor(0.2017, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  24 loss :  tensor(0.1825, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  25 loss :  tensor(0.1620, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  26 loss :  tensor(0.1488, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  27 loss :  tensor(0.1425, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  28 loss :  tensor(0.1442, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  29 loss :  tensor(0.1493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  30 loss :  tensor(0.1486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  31 loss :  tensor(0.1380, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  32 loss :  tensor(0.1328, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  33 loss :  tensor(0.1187, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  34 loss :  tensor(0.1208, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  35 loss :  tensor(0.1238, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  36 loss :  tensor(0.1238, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  37 loss :  tensor(0.1209, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  38 loss :  tensor(0.1158, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  39 loss :  tensor(0.1150, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  40 loss :  tensor(0.1138, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  41 loss :  tensor(0.1094, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  42 loss :  tensor(0.1070, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  43 loss :  tensor(0.1055, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  44 loss :  tensor(0.1036, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  45 loss :  tensor(0.1029, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  46 loss :  tensor(0.0975, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  47 loss :  tensor(0.0964, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  48 loss :  tensor(0.0968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  49 loss :  tensor(0.0952, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  50 loss :  tensor(0.0935, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPIoqkC-Fol",
        "outputId": "09461681-a3f0-460e-876c-9347c3a51d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(text):\n",
        "    #set the model to evaluation\n",
        "    model.eval()\n",
        "\n",
        "    #preprocess\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    #converting the text to word tokens\n",
        "    input_tokens = word_tokenize(text)\n",
        "\n",
        "    #converting the tokens to integer sequence\n",
        "    sequences = tokenizer.texts_to_sequences([input_tokens])\n",
        "\n",
        "    #converting to array\n",
        "    sequences=np.asarray(sequences)\n",
        "    with torch.no_grad():\n",
        "        #converting to tensor\n",
        "        sequences=torch.from_numpy(sequences)\n",
        "        #predicting the output\n",
        "        predict,(hidden,cell)=model(sequences)\n",
        "\n",
        "    #applying the softmax layer\n",
        "    softmax = torch.exp(predict)\n",
        "    prob = list(softmax.numpy())\n",
        "\n",
        "    #index of the predict word\n",
        "    predictions = np.argmax(prob)\n",
        "\n",
        "    #converting the sequence back to word\n",
        "    next_word=tokenizer.sequences_to_texts([[predictions]])\n",
        "    return next_word"
      ],
      "metadata": {
        "id": "JrgT0vm69ULm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we trained our model with sequence length of 2\n",
        "input_text=\"next word\""
      ],
      "metadata": {
        "id": "0ImYSUEP9UPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Possible next word will be:\")\n",
        "predict_next_word(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zrOQBmk9hml",
        "outputId": "095b1a96-b1d7-406b-e6f6-1a8b3dac642a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible next word will be:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['prediction']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we trained our model with sequence length of 2\n",
        "input_text=\"NLP language\""
      ],
      "metadata": {
        "id": "BsxAWp0y9keC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Possible next word will be:\")\n",
        "predict_next_word(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM4xonrB9kuW",
        "outputId": "07f005e6-1f65-43d6-bf79-c20c4614874b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible next word will be:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['of']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}